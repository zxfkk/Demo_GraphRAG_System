import json
import logging
import os
import hashlib
from openai import OpenAI
from config import settings
from core.embedding import get_embeddings_batch

logger = logging.getLogger(__name__)

def get_cache_path(content, key_identifier):
    """
    è®¡ç®—å†…å®¹çš„å“ˆå¸Œå€¼å¹¶è¿”å›ç¼“å­˜æ–‡ä»¶è·¯å¾„
    å¹¶ä¸”ä¼šæ¸…ç†æ‰è¯¥ key_identifier (é€šå¸¸æ˜¯æ–‡ä»¶å) å¯¹åº”çš„æ—§ç¼“å­˜
    """
    # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
    storage_dir = os.path.join(settings.ROOT_DIR, 'storage')
    if not os.path.exists(storage_dir):
        os.makedirs(storage_dir)

    # è®¡ç®—æ–° Hash
    hash_md5 = hashlib.md5(content.encode('utf-8')).hexdigest()
    
    # æ„é€ æ–°çš„ç¼“å­˜æ–‡ä»¶å: è¿™é‡Œçš„ key_identifier å»ºè®®ä¼ å…¥æ–‡ä»¶åï¼Œä¾‹å¦‚ "UE5"
    # æ–‡ä»¶åæ ¼å¼: {æ–‡ä»¶å}.{Hash}.json
    # è¿™æ ·åšçš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥é€šè¿‡å‰ç¼€å¿«é€Ÿæ‰¾åˆ°æ—§ç‰ˆæœ¬ç¼“å­˜
    new_cache_filename = f"{key_identifier}.{hash_md5}.json"
    new_cache_path = os.path.join(storage_dir, new_cache_filename)
    
    # å¦‚æœè¿™ä¸ªç¡®åˆ‡çš„æ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œè¯´æ˜å®Œå…¨æ²¡å˜ï¼Œç›´æ¥è¿”å›
    if os.path.exists(new_cache_path):
        return new_cache_path

    # å¦‚æœè¿™ä¸ªæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯´æ˜å†…å®¹å˜äº†ï¼ˆHashå˜äº†ï¼‰
    # æ­¤æ—¶æˆ‘ä»¬éœ€è¦æ¸…ç†æ‰è¿™ä¸ª key_identifier å¯¹åº”çš„æ‰€æœ‰äººæ—§ç¼“å­˜
    # éå† storage ç›®å½•
    for filename in os.listdir(storage_dir):
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªæ–‡ä»¶çš„æ—§ç¼“å­˜ (ä»¥ key_identifier + "." å¼€å¤´ï¼Œä¸”ä¸æ˜¯å½“å‰è¿™ä¸ªæ–°æ–‡ä»¶)
        if filename.startswith(f"{key_identifier}.") and filename != new_cache_filename:
            old_path = os.path.join(storage_dir, filename)
            try:
                os.remove(old_path)
                logger.info(f"ğŸ§¹ æ¸…ç†æ—§ç¼“å­˜: {filename}")
            except OSError as e:
                logger.warning(f"æ— æ³•åˆ é™¤æ—§ç¼“å­˜ {filename}: {e}")

    return new_cache_path

def extract_hybrid_data(text, prompt_template, source_id="unknown_source"):
    """
    åˆ©ç”¨ LLM æå–ä¸‰å…ƒç»„å’Œå—ä¿¡æ¯
    :param source_id: å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œé€šå¸¸ä¼ æ–‡ä»¶åï¼Œç”¨äºç¼“å­˜ç®¡ç†
    """
    client = OpenAI(api_key=settings.API_KEY, base_url=settings.BASE_URL)
    
    # æ›¿æ¢ Prompt ä¸­çš„å ä½ç¬¦
    if "CONTENT_PLACEHOLDER" not in prompt_template:
        logger.warning("Prompt æ¨¡æ¿ä¸­æœªæ‰¾åˆ° CONTENT_PLACEHOLDERï¼Œå¯èƒ½å¯¼è‡´æå–å¤±è´¥ã€‚")
        
    prompt = prompt_template.replace("CONTENT_PLACEHOLDER", text)

    logger.info("="*15 + f" å¼€å§‹åˆ†ææ–°ç¬”è®° [{source_id}] " + "="*15)
    
    # 1. æ‰“å°è¯¦ç»† Prompt (å‰100å­— + å100å­—)
    if len(prompt) > 200:
        log_prompt = f"{prompt[:100]} ... [çœç•¥ {len(prompt)-200} å­—] ... {prompt[-100:]}"
    else:
        log_prompt = prompt
    logger.info(f"ğŸ“¤ [Request] å‘é€ç»™ API çš„å®é™…å†…å®¹:\n{log_prompt}")
    
    # 2. æ£€æŸ¥ç¼“å­˜ (ä¼ å…¥ source_id)
    cache_file = get_cache_path(prompt, source_id)
    content = ""
    is_cached = False
    
    if os.path.exists(cache_file):
        logger.info(f"ğŸ“¦ æ­¤å†…å®¹å·²åœ¨ storage ä¸­æ‰¾åˆ°ç¼“å­˜ ({os.path.basename(cache_file)})ï¼Œè·³è¿‡ API è°ƒç”¨ã€‚")
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                content = f.read()
            is_cached = True
        except Exception as e:
            logger.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}ï¼Œå‡†å¤‡é‡æ–°è°ƒç”¨ APIã€‚")
            content = ""
    
    # 3. å¦‚æœæ— ç¼“å­˜ï¼Œè°ƒç”¨ API
    if not content:
        try:
            response = client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=settings.TEMPERATURE
            )
            content = response.choices[0].message.content.strip()
            
            # å­˜å…¥ç¼“å­˜
            try:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                logger.info(f"ğŸ’¾ API è¿”å›å€¼å·²ä¿å­˜è‡³ç¼“å­˜: {cache_file}")
            except Exception as e:
                logger.error(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
                
        except Exception as e:
            logger.error(f"âŒ LLM å¤„ç†å‡ºé”™: {str(e)}")
            return [], [], ""

    # 4. æ‰“å°è¯¦ç»† Response
    if len(content) > 200:
        log_content = f"{content[:100]} ... [çœç•¥ {len(content)-200} å­—] ... {content[-100:]}"
    else:
        log_content = content
    logger.info(f"ğŸ“¥ [Response] API è¿”å›è¯¦ç»†å†…å®¹:\n{log_content}")

    try:
        # æ¸…æ´— JSON
        cleaned_content = content
        if cleaned_content.startswith("```"):
            cleaned_content = cleaned_content.replace("```json", "").replace("```", "")
        
        data = json.loads(cleaned_content)
        
        triplets = data.get("triplets", [])
        chunks = data.get("chunks", [])

        # ================= Embedding è¡¥å…¨é€» =================
        # æ£€æŸ¥ chunks ä¸­æ˜¯å¦ç¼ºå¤± embedding
        missing_embeddings_indices = []
        texts_to_embed = []

        for i, chunk in enumerate(chunks):
            if "embedding" not in chunk or not chunk["embedding"]:
                missing_embeddings_indices.append(i)
                texts_to_embed.append(chunk["content"])

        if texts_to_embed:
            logger.info(f"ğŸ§© æ£€æµ‹åˆ° {len(texts_to_embed)} ä¸ªæ–‡æœ¬å—ç¼ºå¤± Embedding (å…± {len(chunks)} ä¸ª)ï¼Œæ­£åœ¨è¡¥å…¨...")
            
            embeddings = get_embeddings_batch(texts_to_embed)
            
            # å¡«å› chunks
            for idx, emb in zip(missing_embeddings_indices, embeddings):
                chunks[idx]["embedding"] = emb
            
            # æ›´æ–° data å¯¹è±¡
            data["chunks"] = chunks
            
            # å›å†™ç¼“å­˜ (è¦†ç›–æ—§æ–‡ä»¶)
            try:
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¦†ç›–å†™å…¥çš„æ˜¯å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œä¸å†æ˜¯ LLM åŸå§‹è¿”å›çš„æ‰€è°“ Markdown å­—ç¬¦ä¸²
                # ä¸ºäº†ä¿æŒé€šè¿‡ "content" å˜é‡çš„ä¸€è‡´æ€§ï¼Œè™½ç„¶ç¨å¾®æ”¹å˜äº†å­˜å‚¨æ ¼å¼(çº¯JSON vs MarkdownåŒ…è£¹)ï¼Œ
                # ä½†ä¸‹ä¸€æ¬¡è¯»å–æ—¶ json.loads ä¾ç„¶èƒ½è§£æçº¯ JSON
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ Embedding å·²è¡¥å…¨å¹¶æ›´æ–°ç¼“å­˜: {cache_file}")
            except Exception as e:
                logger.error(f"âŒ ç¼“å­˜å›å†™å¤±è´¥: {e}")
        else:
            logger.info(f"â© æ‰€æœ‰æ–‡æœ¬å—å‡åŒ…å« Embeddingï¼Œè·³è¿‡å‘é‡è®¡ç®—ã€‚")
        # ===================================================
        
        token_info = " (Cached)" if is_cached else ""
        logger.info(f"âœ… æå–æˆåŠŸ: {len(triplets)} ä¸ªä¸‰å…ƒç»„, {len(chunks)} ä¸ªå—ã€‚{token_info}")
        
        # è¿”å› hash_md5 æ–¹ä¾¿ä¸Šå±‚åšç‰ˆæœ¬æ§åˆ¶
        # æ³¨æ„ï¼šè¿™é‡Œçš„ content æ˜¯ API è¿”å›çš„å†…å®¹ï¼Œä¸æ˜¯åŸå§‹ç¬”è®°å†…å®¹çš„ hashã€‚
        # æˆ‘ä»¬åº”è¯¥è¿”å›åŸå§‹ç¬”è®° prompt çš„ hashï¼Œå³ç”Ÿæˆ cache_file æ—¶ç”¨çš„ hashã€‚
        # ä¸Šé¢ get_cache_path é‡Œç®—è¿‡ä¸€æ¬¡ï¼Œä½†æ²¡æœ‰è¿”å›ã€‚
        # æˆ‘ä»¬é‡æ–°è®¡ç®—ä¸€æ¬¡æˆ–è€…è®© get_cache_path è¿”å›ã€‚
        # ç®€å•èµ·è§ï¼Œé‡æ–°è®¡ç®— prompt çš„ hash (å®ƒåŒ…å«äº†åŸå§‹æ–‡æœ¬)
        current_hash = hashlib.md5(prompt.encode('utf-8')).hexdigest()
        
        return triplets, chunks, current_hash

    except Exception as e:
        logger.error(f"âŒ JSON è§£æå‡ºé”™: {str(e)}", exc_info=True)
        return [], [], ""
