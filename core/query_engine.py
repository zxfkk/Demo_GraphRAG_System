import logging
import json
from openai import OpenAI
from config import settings
from core.neo4j_manager import Neo4jManager
from core.embedding import get_embedding

logger = logging.getLogger(__name__)

class GraphRAGQuery:
    def __init__(self):
        self.neo4j = Neo4jManager()
        self.llm_client = OpenAI(api_key=settings.API_KEY, base_url=settings.BASE_URL)
        
    def query(self, user_query, top_k=5):
        """
        æ‰§è¡Œå®Œæ•´çš„ RAG æ£€ç´¢ä¸ç”Ÿæˆæµç¨‹
        :param user_query: ç”¨æˆ·é—®é¢˜
        :param top_k: æ£€ç´¢å¬å›çš„ chunk æ•°é‡
        """
        if not user_query:
            return "âŒ é—®é¢˜ä¸èƒ½ä¸ºç©º"

        logger.info(f"ğŸ” æ”¶åˆ°æŸ¥è¯¢: {user_query}")

        # 1. é—®é¢˜å‘é‡åŒ–
        query_embedding = get_embedding(user_query)
        if not query_embedding:
            return "âŒ æ— æ³•ç”Ÿæˆé—®é¢˜å‘é‡ï¼Œè¯·æ£€æŸ¥ Embedding æœåŠ¡ã€‚", ""

        # 2. æ··åˆæ£€ç´¢ï¼ˆå‘é‡ç›¸ä¼¼åº¦ + å›¾è°±å…³è”ï¼‰
        # è¿™ä¸€æ­¥é€šè¿‡ Neo4j çš„å‘é‡ç´¢å¼•æŸ¥æ‰¾ç›¸ä¼¼ Chunkï¼Œå¹¶é¡ºå¸¦æŠŠç›¸å…³çš„ Concept åå­—ä¹ŸæŸ¥å‡ºæ¥
        retrieved_info = self._vector_graph_search(query_embedding, top_k)
        
        if not retrieved_info:
            return "âš ï¸ æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", ""

        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context_str = self._format_context(retrieved_info)
        
        # 4. ç”Ÿæˆå›ç­”
        answer, full_prompt = self._generate_answer(user_query, context_str)
        
        return answer, full_prompt

    def direct_chat(self, user_query):
        """
        ç›´æ¥è°ƒç”¨ LLM è¿›è¡Œé—®ç­”ï¼ˆVanilla RAGï¼‰ï¼Œç”¨äºå¯¹æ¯”
        """
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
        full_prompt = f"System: {system_prompt}\nUser: {user_query}"
        
        try:
            response = self.llm_client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query}
                ],
                temperature=settings.TEMPERATURE
            )
            return response.choices[0].message.content, full_prompt
        except Exception as e:
            return f"âŒ ç›´æ¥ç”Ÿæˆå¤±è´¥: {e}", full_prompt

    def _vector_graph_search(self, query_vec, top_k):
        """
        æ ¸å¿ƒæ£€ç´¢é€»è¾‘ï¼š
        1. ä½¿ç”¨ vector index æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ chunk
        2. æ‰¾åˆ° chunk æ‰€å±çš„ subject (å®ä½“)
        3. æŸ¥æ‰¾è¿™äº› subject çš„å…¶ä»–å…³ç³»ä½œä¸ºè¡¥å……ï¼ˆå¯é€‰ï¼Œæš‚æ—¶å…ˆåªå– chunk å†…å®¹å’Œ å®ä½“åï¼‰
        """
        if not self.neo4j.driver:
            return []

        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä¹‹å‰åˆ›å»ºçš„ç´¢å¼•åä¸º chunk_embedding_index
        # ä½¿ç”¨ Neo4j 5.x çš„ db.index.vector.queryNodes è¿‡ç¨‹
        cypher = f"""
        CALL db.index.vector.queryNodes('chunk_embedding_index', $top_k, $query_vec) 
        YIELD node AS chunk, score
        
        // æ‰¾åˆ°è¯¥ chunk å…³è”çš„å®ä½“ï¼ˆä¸»è¯­ï¼‰
        OPTIONAL MATCH (s:Concept)-[:HAS_MENTION|DESCRIBES|RELATED_TO]->(chunk)
        
        RETURN chunk.content AS content, 
               s.name AS entity, 
               score
        """
        try:
            with self.neo4j.driver.session() as session:
                result = session.run(cypher, top_k=top_k, query_vec=query_vec)
                return [record.data() for record in result]
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _format_context(self, records):
        """å°†æ£€ç´¢åˆ°çš„è®°å½•æ ¼å¼åŒ–ä¸º LLM å¯è¯»çš„æ–‡æœ¬"""
        context_parts = []
        for i, rec in enumerate(records):
            content = rec['content']
            entity = rec['entity']
            score = rec['score']
            
            # æ ¼å¼ç¤ºä¾‹:
            # [å‚è€ƒç‰‡æ®µ 1] (ç›¸å…³åº¦: 0.92, å…³è”å®ä½“: é—­åŒ…)
            # å†…å®¹: é—­åŒ…æ˜¯ä¸€ä¸ªå‡½æ•°...
            part = f"[å‚è€ƒç‰‡æ®µ {i+1}] (ç›¸å…³åº¦: {score:.3f}, å…³è”å®ä½“: {entity})\nå†…å®¹: {content}"
            context_parts.append(part)
        
        return "\n\n".join(context_parts)

    def _generate_answer(self, query, context):
        """è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”ï¼Œè¿”å› (answer, full_prompt)"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹æ–¹æä¾›çš„ã€å‚è€ƒä¿¡æ¯ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        å¦‚æœå‚è€ƒä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´æ˜â€œçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹â€ï¼Œä¸è¦ç¼–é€ ã€‚
        å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œå¼•ç”¨ä¿¡æ¯æ—¶è¯·æ³¨æ˜æ¥æºã€‚
        """
        
        user_prompt = f"""
        ã€å‚è€ƒä¿¡æ¯ã€‘
        {context}
        
        ã€ç”¨æˆ·é—®é¢˜ã€‘
        {query}
        """

        full_prompt = f"System: {system_prompt}\nUser: {user_prompt}"
        
        try:
            response = self.llm_client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=settings.TEMPERATURE
            )
            return response.choices[0].message.content, full_prompt
        except Exception as e:
            return f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {e}", full_prompt
