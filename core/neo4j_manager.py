import logging
from neo4j import GraphDatabase
from config import settings

logger = logging.getLogger(__name__)

class Neo4jManager:
    def __init__(self):
        self.driver = None
        self.connect()

    def connect(self):
        """è¿æ¥åˆ° Neo4j æ•°æ®åº“å¹¶åˆå§‹åŒ–çº¦æŸ"""
        try:
            self.driver = GraphDatabase.driver(
                settings.NEO4J_URI, 
                auth=(settings.NEO4J_USER, settings.NEO4J_PASSWORD)
            )
            
            self.driver.verify_connectivity()
            logger.info("âœ… Neo4j è¿æ¥æˆåŠŸï¼")
            
            self.create_constraints()
            
        except Exception as e:
            logger.error(f"âŒ Neo4j è¿æ¥å¤±è´¥: {e}")
            self.driver = None

    def close(self):
        """å…³é—­é©±åŠ¨è¿æ¥"""
        if self.driver:
            self.driver.close()

    def create_constraints(self):
        """åˆ›å»ºå”¯ä¸€æ€§çº¦æŸå’Œç´¢å¼•ï¼Œä¿è¯ConceptèŠ‚ç‚¹çš„nameå±æ€§å”¯ä¸€ï¼ŒChunkèŠ‚ç‚¹çš„contentå±æ€§å”¯ä¸€"""
        if not self.driver:
            return
        
        try:
            with self.driver.session() as session:
                # é’ˆå¯¹ Concept åˆ›å»ºçº¦æŸ 
                session.run("CREATE CONSTRAINT constraint_concept_name IF NOT EXISTS FOR (c:Concept) REQUIRE c.name IS UNIQUE")
                
                # é’ˆå¯¹ Chunk åˆ›å»ºç´¢å¼•
                session.run("CREATE INDEX index_chunk_content IF NOT EXISTS FOR (c:Chunk) ON (c.content)")

                # åˆ›å»ºå‘é‡ç´¢å¼• (é’ˆå¯¹ Chunk çš„ embedding å±æ€§)
                # æ³¨æ„: Neo4j 5.x è¯­æ³•
                try:
                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ (ç®€å•æ£€æŸ¥ï¼Œé˜²æ­¢é‡å¤åˆ›å»ºæŠ¥é”™)
                    # è¿™é‡Œçš„ç»´åº¦å¿…é¡»ä¸ settings.EMBEDDING_DIM ä¸€è‡´
                    vector_index_query = f"""
                    CREATE VECTOR INDEX chunk_embedding_index IF NOT EXISTS
                    FOR (c:Chunk) ON (c.embedding)
                    OPTIONS {{indexConfig: {{
                        `vector.dimensions`: {settings.EMBEDDING_DIM},
                        `vector.similarity_function`: 'cosine'
                    }}}}
                    """
                    session.run(vector_index_query)
                    logger.info("âš¡ å‘é‡ç´¢å¼• check/create å®Œæˆ")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ›å»ºå‘é‡ç´¢å¼•æ—¶é‡åˆ°é—®é¢˜ (å¦‚æœæ˜¯æ—§ç‰ˆæœ¬ Neo4j è¯·å¿½ç•¥): {e}")
            
            logger.info("âš¡ Neo4j ç´¢å¼•/çº¦æŸæ£€æŸ¥å®Œæ¯•")
        except Exception as e:
            logger.info(f"â„¹ï¸ å°è¯•åˆ›å»ºç´¢å¼•/çº¦æŸ: {e}")

    def save_triplets(self, triplets, source_id="unknown"):
        """
        é«˜æ€§èƒ½ä¿å­˜ä¸‰å…ƒç»„ï¼šæŒ‰å…³ç³»ç±»å‹åˆ†ç»„ + UNWIND æ‰¹é‡å†™å…¥
        :param triplets: List[Dict] [{"head":..., "relation":..., "tail":...}]
        :param source_id: æ¥æºæ ‡è¯†
        """
        if not self.driver or not triplets:
            return

        # 1. å†…å­˜åˆ†ç»„
        grouped_data = {}
        for item in triplets:
            rel_type = item["relation"]
            safe_rel_type = "_".join(rel_type.split()).upper()
            if not safe_rel_type:
                safe_rel_type = "RELATED_TO"
                
            if safe_rel_type not in grouped_data:
                grouped_data[safe_rel_type] = []
            
            grouped_data[safe_rel_type].append({
                "h_name": item["head"],
                "t_name": item["tail"],
                "source": source_id
            })

        count = 0
        try:
            with self.driver.session() as session:
                # ä½¿ç”¨äº‹åŠ¡å†™å…¥
                with session.begin_transaction() as tx:
                    for rel_type, batch_data in grouped_data.items():
                        cypher = f"""
                        UNWIND $batch AS row
                        MERGE (h:Concept {{name: row.h_name}})
                        MERGE (t:Concept {{name: row.t_name}})
                        MERGE (h)-[r:`{rel_type}`]->(t)
                        SET r.source = row.source
                        """
                        tx.run(cypher, batch=batch_data)
                        count += len(batch_data)
                    
                    tx.commit()
            
            logger.info(f"ğŸ’¾ [Batch] å·²å‘ Neo4j å­˜å…¥ {count} ä¸ªå…³ç³» (Source: {source_id})")
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¿å­˜ä¸‰å…ƒç»„å¤±è´¥: {e}")

    def save_chunks(self, chunks, source_id="unknown"):
        """
        é«˜æ€§èƒ½ä¿å­˜å—ï¼šUNWIND æ‰¹é‡å†™å…¥
        """
        if not self.driver or not chunks:
            return

        # é¢„å¤„ç†
        batch_data = []
        for item in chunks:
            batch_data.append({
                "content": item["content"],
                "embedding": item.get("embedding", None), # æ–°å¢ embedding å­—æ®µ
                "subject": item["subject"],
                "predicate": item.get("predicate", "HAS_MENTION"),
                "source": source_id
            })

        # åˆ†ç»„
        grouped_chunks = {}
        for item in batch_data:
            pred = "_".join(item["predicate"].split()).upper()
            if pred not in grouped_chunks:
                grouped_chunks[pred] = []
            grouped_chunks[pred].append(item)

        total = 0
        try:
            with self.driver.session() as session:
                with session.begin_transaction() as tx:
                    for pred, batch in grouped_chunks.items():
                        cypher = f"""
                        UNWIND $batch AS row
                        MERGE (s:Concept {{name: row.subject}})
                        CREATE (c:Chunk {{content: row.content, source: row.source}})
                        SET c.embedding = row.embedding  // è®¾ç½®å‘é‡å±æ€§
                        CREATE (s)-[:`{pred}`]->(c)
                        """
                        tx.run(cypher, batch=batch)
                        total += len(batch)
                    tx.commit()
            
            logger.info(f"ğŸ“„ [Batch] å·²å‘ Neo4j å­˜å…¥ {total} ä¸ªæ–‡æœ¬å—èŠ‚ç‚¹")
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¿å­˜ Chunk å¤±è´¥: {e}")

    def prune_source_data(self, source_id):
        """
        åœ¨å†™å…¥æ–°æ•°æ®å‰ï¼Œæ¸…ç†è¯¥ source_id å¯¹åº”çš„æ—§æ•°æ®ï¼ˆChunk å’Œ å…³ç³»ï¼‰
        æ³¨æ„ï¼šä¸åˆ é™¤ Concept èŠ‚ç‚¹ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½æ˜¯å…¬ç”¨çš„
        """
        if not self.driver or not source_id:
            return

        try:
            with self.driver.session() as session:
                # 1. åˆ é™¤è¯¥æ¥æºçš„æ‰€æœ‰ Chunk èŠ‚ç‚¹ (DETACH DELETE ä¼šè‡ªåŠ¨åˆ é™¤è¿æ¥çš„å…³ç³»)
                session.run("MATCH (c:Chunk {source: $source}) DETACH DELETE c", source=source_id)
                
                # 2. åˆ é™¤è¯¥æ¥æºçš„æ‰€æœ‰å…³ç³» (ä¹Ÿå°±æ˜¯ Triplets å»ºç«‹çš„å…³ç³»)
                # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåˆ é™¤å±æ€§ source = current_source çš„æ‰€æœ‰è¾¹
                session.run("MATCH ()-[r]-() WHERE r.source = $source DELETE r", source=source_id)
                
            logger.info(f"ğŸ§¹ å·²æ¸…ç†æ—§æ•°æ® (Source: {source_id})")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")

    def get_source_hash(self, source_id):
        """è·å–æŒ‡å®šæºåœ¨æ•°æ®åº“ä¸­å­˜å‚¨çš„ Hash ç‰ˆæœ¬"""
        if not self.driver or not source_id:
            return None
        try:
            with self.driver.session() as session:
                result = session.run(
                    "MATCH (m:SourceMetadata {id: $id}) RETURN m.hash AS hash LIMIT 1",
                    id=source_id
                ).single()
                return result["hash"] if result else None
        except Exception:
            return None

    def update_source_hash(self, source_id, new_hash):
        """æ›´æ–°æºçš„ Hash ç‰ˆæœ¬"""
        if not self.driver or not source_id:
            return
        try:
            with self.driver.session() as session:
                session.run(
                    "MERGE (m:SourceMetadata {id: $id}) SET m.hash = $hash",
                    id=source_id, hash=new_hash
                )
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å…ƒæ•°æ®å¤±è´¥: {e}")

    def clear_database(self):
        """å±é™©æ“ä½œï¼šæ¸…ç©ºæ•°æ®åº“"""
        if self.driver:
            try:
                with self.driver.session() as session:
                    session.run("MATCH (n) DETACH DELETE n")
                logger.warning("âš ï¸ æ•°æ®åº“å·²æ¸…ç©ºï¼")
            except Exception as e:
                logger.error(f"æ¸…ç©ºå¤±è´¥: {e}")
